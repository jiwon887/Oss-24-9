{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,  VisionEncoderDecoderModel, AutoProcessor\n",
    "import torch\n",
    "import cv2\n",
    "import pytesseract\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"jinhybr/OCR-Donut-CORD\")\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"jinhybr/OCR-Donut-CORD\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  \n",
    "model = model.to(device) \n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # 여러 개의 공백을 하나로 줄이고, 양쪽 공백 제거\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # 예를 들어, 특정 패턴 제거하기\n",
    "    text = re.sub(r'<[^>]+>', '', text)  # HTML 태그와 같은 패턴 제거\n",
    "    \n",
    "    return text\n",
    "\n",
    "def extract_from_image(image_path):\n",
    "    \n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    processor = AutoProcessor.from_pretrained(\"jinhybr/OCR-Donut-CORD\")\n",
    "\n",
    "    pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "    pixel_values = pixel_values.to(device) \n",
    "\n",
    "    generated_ids = model.generate(pixel_values)\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)[0]\n",
    "\n",
    "    image = cv2.imread(image_path)\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    _, thresh_image = cv2.threshold(gray_image, 128, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    extracted_text = pytesseract.image_to_string(thresh_image, lang='kor')\n",
    "\n",
    "    extracted_text = clean_text(extracted_text)\n",
    "    \n",
    "    whole_text = f\"{generated_text} {extracted_text}\"\n",
    "\n",
    "    return whole_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s_cord-v2><s_menu><s_nm> BECK.</s_nm><s_num> JLB 01 Hg</s_nm><s_num> J梨</s_num><s_price> JICH.<sep/><s_nm> LH BALONI CHEE LATSULE</s_nm><s_num> JPGS</s_num><s_price> BEL.<sep/><s_nm> WHEN SHORT & HAPPYE G G HOTHOE GEESE HOTHONG HOTHONG HOTHONG HOTHONG HOTHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHONGHO 성 0 10 309 인 소석 뜨 콕 쁘 꾸 공 은 여 기 서 멈 춘 다 . 지 금 이 순 간 부 터 나는 나 의 과 거에 대 하 여 총 체 적 인 잭 임 을 진 다 . 나는 지 혜 의 시 작 이 내 문 제에 대 한 책 임 을 받 아 들 이는 것 ' 임 을 안 다 . 내 과 거에 대 하 여 책 임 을 짐 으 로 써 나는 나 자 산 을 과 거 로 부 터 해 방 시 킬 수 있 다 . 내 가 스 스 로 선 택 한 더 크 고 밝 은 미 래 로 나 아 갈 수 있 다 . 나는 맞 으 로 나 의 현 재 삼 황 에 대 하 여 그 누 구 애 게 도 책 임 을 전 가 하 지 많 겠 다 . 냐 의 교 육 배 경 , 나 의 유 전 자 , 일 삼 생 활 의 다 양 한 여 건이 나 의 미 래에 부 정 적 인 영 향 을 주 지 많 도록 하 겠 다 . 내 가 성 공 하 지 못 한 이 유 를 이런 통 제 하 기 어 려운 함 들 에 미 룬 다 면 , 나는 과 거 의 거 미 줄 에 사 로 잡 혀 영 원 히 빠 져 냐 오 지 못 할 것 미 다 . 나는 맞 을 내 다 보 겠 다 . 나 의 과 거 가 나 의 운 명 을 지 배 하 도록 내 버 려 두 지 많 겠 다 , 공 은 여 기 서 멈 춘 다 . 나는 내 과 거 에 대 하 여 모 든 책 임 을 진 다 . 냐 는 내 성 공 에 대 해 서 도 책 임 을 지 겠 다 . 내 가 오 늘 날 삼 리 적 으 로 , 육 체 적 으 로 , 정 신 적 으 로 . 재 정 적 으 로 이 렇 게 된 것 은 내 가 선 택 한 걸 단 의 결 과 이 다 . 나 의 결 단 은 언 제 나 나 의 선 택 에 의 해 좌 우 된 다 . 나는 나 의 사 고 방 식 을 바 꿈 으 로 써 늘 적 극 적 인 방 향 을 지 향 하 고 , 파 괴 적 인 방 향 은 거 들 떠 보 지도 않 겠 다 . 냐 의 마 음 은 미 래 의 해 결 안 을 응 시 하 고 , 과 거 의 문 제 에 는 더 이상 집 착 하 지 않 겠 다 . 나는 이 세 상에 긍 정 적 인 변 화 를 가 져 오 려 고 애 쓰 는 사 람 들과 사 귀 려 고 노 력 하 겠 다 . 나는 편 안 한 것 만 을 추 구 하 는 사 람 들과 어 울 려 편 안 한 것 만 주 구 하 는 방 식 은 철 저 히 배 제 하 겠 다 . 결 단 을 대 려 야 할 상 황 이 되 먼 반 드 시 결 단 을 내 리 겠 다 . 하 느 님 께 서 나 에 게 늘 돌 바 른 결 단 을 대 릴 수 있 는 등 력 을 주 셨 다 고 생 각 하 지 않 는 다 . 하 지만 일 단 결 단 을 내 리 는 능 력 과 , 또 잘 못 된 결 단 을 내 렸 을 경 우 그 것 을 시 정 하 는 능 력 은 주 셨 다 고 생 각 한 다 . 감 정 의 기 복 에 따 라 나 의 정 해 진 노 선 을 벗 어 나는 일 은 결 코 없 을 것 이 다 . 일 단 결 단 을 내 리 면 끝 까 지 그 것 을 밀 어 붙 일 것 이 다 . 나 의 모 든 정 성 을 기 울 여 그 결 단 사 항 을 실 현 하 려 할 것 이 다 . 공 은 여 기 서 멈 춘 다 . 나는 내 생 각 과 내 감 정 을 통 제 한 다 .\n"
     ]
    }
   ],
   "source": [
    "image_path = r\"C:\\Users\\82102\\Desktop\\CV\\text.jpg\"\n",
    "txt = extract_from_image(image_path)\n",
    "print(txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "요약 결과: WHEN SHORT & HAPPYE G TH ONGOLD ESELL BECK.<sep/><s_nm> JLB 01 MGS LATSULE</sritem.html.kr PHP COUNTER INDEXTREMANY NAVILITALTIC ARRIGHTM ROADISTY.Kr.Htm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"noahkim/KoT5_news_summarization\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"noahkim/KoT5_news_summarization\")\n",
    "\n",
    "\n",
    "# 입력 텍스트를 토큰화\n",
    "input_ids = tokenizer.encode(txt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "# 요약 생성\n",
    "summary_ids = model.generate(\n",
    "    input_ids,\n",
    "    num_beams=4,               # 빔 검색\n",
    "    max_length=150,            # 요약 최대 길이\n",
    "    min_length=30,             # 요약 최소 길이\n",
    "    length_penalty=2.0,        # 길이 패널티\n",
    "    early_stopping=True,       # 조기 종료\n",
    "    no_repeat_ngram_size=2     # 반복되는 n-그램 방지\n",
    ")\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"요약 결과:\", summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
